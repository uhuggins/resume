}')
wiki2<- query_wikidata('SELECT ?country ?countryLabel ?iso2 ?iso3 WHERE
{
?country wdt:P31 wd:Q6256.
OPTIONAL { ?country wdt:P298 ?iso3. }
SERVICE wikibase:label { bd:serviceParam wikibase:language "[AUTO_LANGUAGE],en"}
}')
View(wiki2)
allWiki<rbind(wiki,wiki2)
allWiki<-rbind(wiki,wiki2)
aw<- istinct(allWiki)
aw<- distinct(allWiki)
View(aw)
library(WikidataQueryServiceR)
wiki<- query_wikidata('SELECT ?country ?countryLabel ?iso3 WHERE
{
?country wdt:P31 wd:Q3624078.
OPTIONAL { ?country wdt:P298 ?iso3. }
SERVICE wikibase:label { bd:serviceParam wikibase:language "[AUTO_LANGUAGE],en"}
}')
wiki2<- query_wikidata('SELECT ?country ?countryLabel ?iso3 WHERE
{
?country wdt:P31 wd:Q6256.
OPTIONAL { ?country wdt:P298 ?iso3. }
SERVICE wikibase:label { bd:serviceParam wikibase:language "[AUTO_LANGUAGE],en"}
}')
allWiki<-rbind(wiki,wiki2)
aw<- distinct(allWiki)
#write.xlsx(joinWiki, file="cia_country-comparison.xlsx", sheetName="data1",  append=TRUE,row.names=TRUE)
View(aw)
w3<- aw %>% mutate(wikidataID = gsub("http://www.wikidata.org/entity/", "", country)) %>%
select(iso3,  countryLabel, wikidataID) %>% filter(!is.na(iso3)) %>% filter(countryLabel != "Danish Realm")
w3<- distinct(w3) %>% select(-countryLabel)
joinWiki<- all %>% left_join(w3, by = c("iso3") ) %>% select(iso3, Country, wikidataID, kep)
View(joinWiki)
View(joinWiki)
View(metadata)
write.xlsx(metadata, file="cia_country-comparison.xlsx", sheetName="meta", row.names=TRUE)
gllimpse(joinWiki)
glimpse(joinWiki)
joinWiki$Reserves of foreign exchange and gold
joinWiki$`Reserves of foreign exchange and gold`
as.numeric(joinWiki$`Reserves of foreign exchange and gold`)
as.numeric(joinWiki$`Refined petroleum products - production` )
sub
sub<- metadata$weblink[[4]]
print(sub)
View(metadata)
sub<- metadata$weblink[[19]]
aub
url <-sub
page <- read_html(url)
nodeslist <- html_nodes(page, ".content-table-container .content-table .header-row th")
unit<-nodeslist %>% html_text()
unit<- unit[[3]]
descList <- html_nodes(page, "div.wysiwyg-wrapper.hero-content")
desc<-descList %>% html_text()
if (length(desc) ==0){
desc <- ''
}
txtlink<-sub
webpage <- read_html(txtlink)
tbls <- html_nodes(webpage, "table")%>%
html_table(fill = TRUE)
df<- tbls[[1]]
names(df)[[3]]<- z$variable
View(df)
df[[3]]<- gsub("[^0-9.-]", "", df[[3]])
glimpse(df)
getData<- function(z){
txtlink<-z$weblink
webpage <- read_html(txtlink)
tbls <- html_nodes(webpage, "table")%>%
html_table(fill = TRUE)
df<- tbls[[1]]
df[[3]]<- gsub("[^0-9.-]", "", df[[3]])
df[[3]]<- as.numeric(df[[3]])
names(df)[[3]]<- z$variable
names(df)[[4]]<- paste0("date_", snakecase::to_lower_camel_case(z$variable) )
df<-df %>% select(-Rank)
}
dfs<- lapply(split(metadata, metadata$variable), getData)
all<- dfs %>% reduce(full_join, by = "Country")
kep<- names(all)
all$iso3<- countrycode(all$Country,  'country.name', "iso3c")
w3<- aw %>% mutate(wikidataID = gsub("http://www.wikidata.org/entity/", "", country)) %>%
select(iso3,  countryLabel, wikidataID) %>% filter(!is.na(iso3)) %>% filter(countryLabel != "Danish Realm")
w3<- distinct(w3) %>% select(-countryLabel)
joinWiki<- all %>% left_join(w3, by = c("iso3") ) %>% select(iso3, Country, wikidataID, kep)
View(joinWiki)
library(xlsx)
glimpse(joinWiki)
View(joinWiki)
txtlink<- 'https://www.cia.gov/the-world-factbook/field/carbon-dioxide-emissions-from-consumption-of-energy/country-comparison'
webpage <- read_html(txtlink)
tbls <- html_nodes(webpage, "table")%>%
html_table(fill = TRUE)
df<- tbls[[1]]
glimpse(df)
df[[3]]<- as.numeric(df[[3]])
glimpse(df)
glimpse(joinWiki)
txtlink<- 'https://www.cia.gov/the-world-factbook/field/total-fertility-rate/country-comparison'
webpage <- read_html(txtlink)
webpage <- read_html(txtlink)
tbls <- html_nodes(webpage, "table")%>%
html_table(fill = TRUE)
df<- tbls[[1]]
glimpse(df)
df[[3]]<- gsub("[^0-9.-]", "", df[[3]])
df[[3]]<- as.numeric(df[[3]])
glimpse(df)
df[[3]]<- gsub("\\..*", "", df[[3]])
df[[3]]<- gsub("[^0-9.-]", "", df[[3]])
df[[3]]<- as.numeric(df[[3]])
glimpse(df)
df<- tbls[[1]]
class(v)
class(df[[3]])
df<- tbls[[1]]
if(class(df[[3]])!="numeric"){
df[[3]]<- gsub("\\..*", "", df[[3]])
df[[3]]<- gsub("[^0-9.-]", "", df[[3]])
df[[3]]<- as.numeric(df[[3]])
}
glimpse(df)
webpage<- read_html('https://www.cia.gov/the-world-factbook/field/taxes-and-other-revenues/country-comparison')
tbls <- html_nodes(webpage, "table")%>%
html_table(fill = TRUE)
df<- tbls[[1]]
if(class(df[[3]])!="numeric"){
df[[3]]<- gsub("\\..*", "", df[[3]])
df[[3]]<- gsub("[^0-9.-]", "", df[[3]])
df[[3]]<- as.numeric(df[[3]])
}
glimpse(df)
webpage<- read_html('https://www.cia.gov/the-world-factbook/field/taxes-and-other-revenues/country-comparison')
tbls <- html_nodes(webpage, "table")%>%
html_table(fill = TRUE)
df<- tbls[[1]]
glimpse(df)
if(class(df[[3]])!="numeric"){
df[[3]]<- gsub("\\..*", "", df[[3]])
df[[3]]<- gsub("[^0-9.-]", "", df[[3]])
df[[3]]<- as.numeric(df[[3]])
}
glimpse(df)
webpage<- read_html('https://www.cia.gov/the-world-factbook/field/carbon-dioxide-emissions-from-consumption-of-energy/country-comparison')
tbls <- html_nodes(webpage, "table")%>%
html_table(fill = TRUE)
df<- tbls[[1]]
glimpse(df)
if(class(df[[3]])!="numeric"){
df[[3]]<- gsub("\\..*", "", df[[3]])
df[[3]]<- gsub("[^0-9.-]", "", df[[3]])
df[[3]]<- as.numeric(df[[3]])
}
glimpse(df)
webpage<- read_html('https://www.cia.gov/the-world-factbook/field/gdp-per-capita-ppp/country-comparison')
tbls <- html_nodes(webpage, "table")%>%
html_table(fill = TRUE)
df<- tbls[[1]]
glimpse(df)
if(class(df[[3]])!="numeric"){
df[[3]]<- gsub("\\..*", "", df[[3]])
df[[3]]<- gsub("[^0-9.-]", "", df[[3]])
df[[3]]<- as.numeric(df[[3]])
}
glimpse(df)
getData<- function(z){
txtlink<-z$weblink
webpage <- read_html(txtlink)
tbls <- html_nodes(webpage, "table")%>%
html_table(fill = TRUE)
df<- tbls[[1]]
glimpse(df)
if(class(df[[3]])!="numeric"){
df[[3]]<- gsub("\\..*", "", df[[3]])
df[[3]]<- gsub("[^0-9.-]", "", df[[3]])
df[[3]]<- as.numeric(df[[3]])
}
glimpse(df)
names(df)[[3]]<- z$variable
names(df)[[4]]<- paste0("date_", snakecase::to_lower_camel_case(z$variable) )
df<-df %>% select(-Rank)
}
getData<- function(z){
txtlink<-z$weblink
webpage <- read_html(txtlink)
tbls <- html_nodes(webpage, "table")%>%
html_table(fill = TRUE)
df<- tbls[[1]]
glimpse(df)
if(class(df[[3]])!="numeric"){
df[[3]]<- gsub("\\..*", "", df[[3]])
df[[3]]<- gsub("[^0-9.-]", "", df[[3]])
df[[3]]<- as.numeric(df[[3]])
}
glimpse(df)
names(df)[[3]]<- z$variable
names(df)[[4]]<- paste0("date_", snakecase::to_lower_camel_case(z$variable) )
df<-df %>% select(-Rank)
df
}
dfs<- lapply(split(metadata, metadata$variable), getData)
all<- dfs %>% reduce(full_join, by = "Country")
kep<- names(all)
library(countrycode)
all$iso3<- countrycode(all$Country,  'country.name', "iso3c")
all$iso3<- countrycode(all$Country,  'country.name', "iso3c")
w3<- aw %>% mutate(wikidataID = gsub("http://www.wikidata.org/entity/", "", country)) %>%
select(iso3,  countryLabel, wikidataID) %>% filter(!is.na(iso3)) %>% filter(countryLabel != "Danish Realm")
w3<- distinct(w3) %>% select(-countryLabel)
joinWiki<- all %>% left_join(w3, by = c("iso3") ) %>% select(iso3, Country, wikidataID, kep)
library(xlsx)
glimpse(joinWiki)
write.xlsx(joinWiki, file="cia_country-comparison.xlsx", sheetName="data",  append=TRUE,row.names=TRUE)
joinWiki %>% filter(country == "India") %>% select("Education Expenditures")
joinWiki %>% filter(Country == "India") %>% select("Education Expenditures")
joinWiki %>% filter(Country == "India") %>% select("Education expenditures")
knitr::opts_chunk$set(echo = TRUE)
# https://www.cia.gov/library/publications/resources/the-world-factbook/docs/rankorderguide.html
library(rio)
data
ranks<-import("cia-country-comparison.xlsx", sheet="data")
ranks<-import("cia-country-comparison.xlsx", sheet="data")
ranks<-import("cia_country-comparison.xlsx", sheet="data")
connect()
source('/Users/umahuggins/Dropbox/ON/graphs_ontopic/connect.R')
library(DBI)
connect()
dbWriteTable(con, Id(schema="cia", table="country-comparison"), travel)
dbWriteTable(con, Id(schema="cia", table="country-comparison"), ranks)
names(ranks)<- snakecase::to_any_case(names(ranks))
View(ranks)
dbWriteTable(con, Id(schema="cia", table="country-comparison"), ranks)
dbWriteTable(con, Id(schema="cia", table="comparison"), ranks)
setwd("~/")
knitr::opts_chunk$set(echo = TRUE)
# https://www.cia.gov/library/publications/resources/the-world-factbook/docs/rankorderguide.html
source('/Users/umahuggins/Dropbox/ON/graphs_ontopic/connect.R')
t1<- dbReadTable(con, Id(schema="cia", table="travel"))
library(DBI)
connect()
t1<- dbReadTable(con, Id(schema="cia", table="travel"))
comp<- dbReadTable(con, Id(schema="cia", table="comparison"))
setdiff(comp$iso_3, t1$iso3)
View(t1)
iris
iris <- iris %>% filter(Sepal.Length > 5 && < Sepal.Width) %>%
summarise(avg = mean(Petal.Width))
library(dplyr)
iris <- iris %>% filter(Sepal.Length > 5 && < Sepal.Width) %>%
summarise(avg = mean(Petal.Width))
library(dplyr)
iris <- iris %>% filter(Sepal.Length > 5 && < Sepal.Width) %>%
summarise(avg = mean(Petal.Width))
names(iris)
answer <- iris %>% filter(Sepal.Length > 5 && < Sepal.Width) %>%
summarise(avg = mean(Petal.Width))
answer <- iris %>% filter(Sepal.Length > 5 && < Sepal.Width)
answer <- iris %>% filter(Sepal.Length > 5 && 3 < Sepal.Width) %>%
summarise(avg = mean(Petal.Width))
View(answer)
##CSV Link
data<- import("https://files.zillowstatic.com/research/public_v2/zhvi/Neighborhood_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_mon.csv")
library(rio)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(ggridges)
library(rio)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(ggridges)
library(rio)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(ggridges)
##CSV Link
data<- import("https://files.zillowstatic.com/research/public_v2/zhvi/Neighborhood_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_mon.csv")
# LENGTHEN DATA
f1<-data %>%
pivot_longer(c(-RegionID,-SizeRank,-RegionName, -RegionType, -StateName, -State,-City, -Metro, -CountyName),
names_to = "date",
values_to = "value")
f1<- f1 %>% mutate(year1 = rev(as.factor(year(date))))
NYC<- filter(f1, City == "New York") ## get only NYC data
#allHoods<- data.frame(hoodd = unique(NYC$RegionName)) ##SEE ALL NEIGHBORHOODS
##Choose some cool neighborhoods
hoods<- c("East Village", "Upper West Side", "Williamsburg", "Park Slope", "Astoria", "Bushwick")
single<- NYC %>% filter(RegionName=='Bushwick') %>% filter(!is.na(value))
## FACET BY NEIGHBORHOOD
manyHoods<- NYC %>% filter(RegionName %in%  hoods) %>% filter(!is.na(value)) %>%
select(year1, RegionName, value  )
manyHoods$value2<- manyHoods$value/1000
manyHoods %>%
ggplot(aes(y = year1)) +
geom_density_ridges(
aes(x = value2, fill = paste(year1, RegionName))
# alpha = .8, color = "white", from = 0, to = 1000000
) +
labs(
x = "ZHVI - thousands",
y = "Year",
title = "NYC Neighborhoods",
subtitle = "since 1996",
caption = "Source: Zillow"
) + scale_x_continuous(expand = c(0, 0)) +scale_y_continuous(trans = "reverse")+
scale_fill_cyclical(
breaks = c("2003 East Village", "2003 Upper West Side", "2003 Williamsburg", "2003 Park Slope", "2003 Astoria", "2003 Bushwick"),
labels = c(`2003 East Village` = "East Village",
`2003 Upper West Side` = "Upper West Side",
`2003 Williamsburg` =  "Williamsburg",
`2003 Park Slope` =  "Park Slope",
`2003 Astoria` =  "Astoria",
`2003 Bushwick` = "Bushwick"),
values = c( "#ffffff", "#cb8dff", "#9900f0",  "#000000", "#7974f1", "#7d9bb3"),
name = "Neighborhood", guide = "legend") +
coord_cartesian(clip = "off") +
theme_ridges(grid = FALSE)
# LENGTHEN DATA
f1<-data %>%
pivot_longer(c(-RegionID,-SizeRank,-RegionName, -RegionType, -StateName, -State,-City, -Metro, -CountyName),
names_to = "date",
values_to = "value")
f1<- f1 %>% mutate(year1 =  as.factor(year(date)),
year1 = factor(year1, levels = rev(levels(year1))))
NYC<- filter(f1, City == "New York") ## get only NYC data
library(highcharter)
hchart(f1$year1)
## FACET BY NEIGHBORHOOD
manyHoods<- NYC %>% filter(RegionName %in%  hoods) %>% filter(!is.na(value)) %>%
select(year1, RegionName, value  )
manyHoods$value2<- manyHoods$value/1000
manyHoods %>%
ggplot(aes(y = year1)) +
geom_density_ridges(
aes(x = value2, fill = paste(year1, RegionName))
# alpha = .8, color = "white", from = 0, to = 1000000
) +
labs(
x = "ZHVI - thousands",
y = "Year",
title = "NYC Neighborhoods",
subtitle = "since 1996",
caption = "Source: Zillow"
) + scale_x_continuous(expand = c(0, 0)) +scale_y_continuous(trans = "reverse")+
scale_fill_cyclical(
breaks = c("2003 East Village", "2003 Upper West Side", "2003 Williamsburg", "2003 Park Slope", "2003 Astoria", "2003 Bushwick"),
labels = c(`2003 East Village` = "East Village",
`2003 Upper West Side` = "Upper West Side",
`2003 Williamsburg` =  "Williamsburg",
`2003 Park Slope` =  "Park Slope",
`2003 Astoria` =  "Astoria",
`2003 Bushwick` = "Bushwick"),
values = c( "#ffffff", "#cb8dff", "#9900f0",  "#000000", "#7974f1", "#7d9bb3"),
name = "Neighborhood", guide = "legend") +
coord_cartesian(clip = "off") +
theme_ridges(grid = FALSE)
manyHoods %>%
ggplot(aes(y = year1)) +
geom_density_ridges(
aes(x = value2, fill = paste(year1, RegionName))
# alpha = .8, color = "white", from = 0, to = 1000000
) +
labs(
x = "ZHVI - thousands",
y = "Year",
title = "NYC Neighborhoods",
subtitle = "since 1996",
caption = "Source: Zillow"
) + scale_x_continuous(expand = c(0, 0)) +
scale_fill_cyclical(
breaks = c("2003 East Village", "2003 Upper West Side", "2003 Williamsburg", "2003 Park Slope", "2003 Astoria", "2003 Bushwick"),
labels = c(`2003 East Village` = "East Village",
`2003 Upper West Side` = "Upper West Side",
`2003 Williamsburg` =  "Williamsburg",
`2003 Park Slope` =  "Park Slope",
`2003 Astoria` =  "Astoria",
`2003 Bushwick` = "Bushwick"),
values = c( "#ffffff", "#cb8dff", "#9900f0",  "#000000", "#7974f1", "#7d9bb3"),
name = "Neighborhood", guide = "legend") +
coord_cartesian(clip = "off") +
theme_ridges(grid = FALSE)
devtools::install_github("ctzn-pub/zSocial")
getwd()
searchUS(c("Trump", "Biden"), "politics" )
library(zSocial)
searchUS(c("Trump", "Biden"), "politics" )
getwd()
library(knitr)
library(rmdformats)
## Global options
options(max.print="75")
opts_chunk$set(echo=FALSE,
cache=TRUE,
prompt=FALSE,
tidy=TRUE,
comment=NA,
message=FALSE,
warning=FALSE)
opts_knit$set(width=75)
getwd()
library(knitr)
library(rmdformats)
## Global options
options(max.print="75")
opts_chunk$set(echo=FALSE,
cache=TRUE,
prompt=FALSE,
tidy=TRUE,
comment=NA,
message=FALSE,
warning=FALSE)
opts_knit$set(width=75)
election<-import('/Users/umahuggins/Downloads/precincts-with-results.geojson')
election<-rio::import('/Users/umahuggins/Downloads/precincts-with-results.geojson')
library(sf)
gjsf = st_read(election)
gjsf = st_read('/Users/umahuggins/Downloads/precincts-with-results.geojson')
gjsf$geometry
gjsf$GEOID
install.packages()
library(devtools)
install_github("ctzn-pub/zFormats")
getwd()
list.fies()
list.files()
remove.packages('zTemplates')
remove.packages('zTemplate')
# install.packages("devtools")
devtools::install_github("etiennebacher/docsifier")
librarry(bookdown)
library(bookdown)
devtools::install_github("ctzn-pub/zFormats")
library(highcharter)
highcharter::list_parse2()
library(docsifier)
knitr::opts_chunk$set(echo = TRUE)
library("gallerywidget")
library("tibble")
gallerywidget(data_htmlwidgets)
knitr::opts_chunk$set(echo = TRUE)
library(googleCloudRunner)
cr_setup()
cr_setup()
cr_setup()
cr_setup()
getwd()
devtools::install_github("nstrayer/datadrivencv")
ssetwd("Rlang")
setwd("Rlang")
datadrivencv::use_datadriven_cv(full_name = "My Name")
setwd("resume")
datadrivencv::use_datadriven_cv(full_name = "My Name")
# Knit the HTML version
rmarkdown::render("cv.rmd",
params = list(pdf_mode = FALSE),
output_file = "cv.html")
# Knit the PDF version to temporary html location
tmp_html_cv_loc <- fs::file_temp(ext = ".html")
rmarkdown::render("cv.rmd",
params = list(pdf_mode = TRUE),
output_file = tmp_html_cv_loc)
# Convert to PDF using Pagedown
pagedown::chrome_print(input = tmp_html_cv_loc,
output = "cv.pdf")
# Knit the HTML version
rmarkdown::render("cv.rmd",
params = list(pdf_mode = FALSE),
output_file = "cv.html")
knitr::opts_chunk$set(
results='asis',
echo = FALSE
)
library(magrittr) # For the pipe
source("cv_printing_functions.r")
# Read in all data and initialize a CV printer object
CV <- create_CV_object(
data_location = "/Library/Frameworks/R.framework/Versions/4.0/Resources/library/datadrivencv/sample_data/",
pdf_mode = params$pdf_mode
)
# Read in all data and initialize a CV printer object
CV <- create_CV_object(
data_location = "files:///Library/Frameworks/R.framework/Versions/4.0/Resources/library/datadrivencv/sample_data/",
pdf_mode = params$pdf_mode
)
.Library
list.files('Library/Frameworks/R.framework/Versions/4.0/Resources/library/datadrivencv/sample_data/')
# Read in all data and initialize a CV printer object
CV <- create_CV_object(
data_location = "/sample_data/",
pdf_mode = params$pdf_mode
)
# Read in all data and initialize a CV printer object
CV <- create_CV_object(
data_location = "sample_data/",
pdf_mode = params$pdf_mode
)
# install.packages("devtools")
devtools::install_github("nstrayer/datadrivencv")
# run ?datadrivencv::use_datadriven_cv to see more details
datadrivencv::use_datadriven_cv(
full_name = "Nick Strayer",
data_location = "https://docs.google.com/spreadsheets/d/14MQICF2F8-vf8CKPF1m4lyGKO6_thG-4aSwat1e2TWc",
pdf_location = "https://github.com/nstrayer/cv/raw/master/strayer_cv.pdf",
html_location = "nickstrayer.me/cv/",
source_location = "https://github.com/nstrayer/cv"
)
